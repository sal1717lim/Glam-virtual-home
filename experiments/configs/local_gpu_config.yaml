lamorel_args:
  log_level: info
  allow_subgraph_use_whith_gradient: false
  distributed_setup_args:
    n_rl_processes: 1
    n_llm_processes: 1
  accelerate_args:
    config_file: ../configs/accelerate/default_config.yaml
    machine_rank: 0
    main_process_ip: 127.0.0.1
    num_machines: 1
  llm_args:
    model_type: seq2seq
    model_path: t5-small
    pretrained: true
    minibatch_size: 192
    pre_encode_inputs: true
    load_in_4bit: False
    parallelism:
      use_gpu: true
      model_parallelism_size: 1
      synchronize_gpus_after_scoring: false
      empty_cuda_cache_after_scoring: false
rl_script_args:
  path: ???
  seed: 1
  # ppo
  ppo_epochs: 4
  lam: 0.99
  gamma: 0.99
  lr: 1e-6
  entropy_coef: 0.01
  value_loss_coef: 0.5
  clip_eps: 0.2
  max_grad_norm: 0.5
  minibatch_size: 8
  use_all_params_for_optim: false
  quantized_optimizer: false
  # llm
  use_lora: False
  gradient_batch_size: 1
  gradient_minibatch_size:
  ## LoRA
  lora_r: 16
  lora_alpha: 32
  lora_target_modules:  # ["q", "v", "k", "wi", "wo", "lm_head"]
  # rl training
  number_envs: 4
  max_ep_len: 200
  epochs: 1000
  steps_per_epoch: 1000
  save_freq: 100
  output_dir: ???
  loading_path: #'/home/cromac/Documents/Projects/PhD/Grounding_LLMs/Large-Scale_Grounding_LLMs_with_online_RL/outputs/results/BabyAIMixed_FlanT5-780M/seed_1/epochs_400-420/'
  transitions_buffer_len: 3
  # environment
  name_environment: 'VirtualHome-v2' # 'babyai_text', 'alfworld'
  task: 'BabyAI-MixedTrainLocal-v0'
  ## BabyAI-specific
  action_space: ["turn_left","turn_right","go_forward","pick_up","drop","toggle"]
  ## AlfWorld-specific
  config_file: '/home/cromac/Documents/Projects/PhD/Grounding_LLMs/alfworld/configs/base_config.yaml'
  train_mode: true